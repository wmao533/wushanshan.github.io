---
layout: page
title: Shanshan Wu
---
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
     <td width="67%" valign="middle">
        <p>
        I am a PhD student in the <a href="https://wncg.org">WNCG</a> at the <a href="http://www.utexas.edu">University of Texas at Austin</a>. I am fortunate to have <a href="http://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a> and <a href="http://users.ece.utexas.edu/~sanghavi/">Sujay Sanghavi</a> as my advisors. I got my BS degree in 2011 and MS degree in 2014 from <a href="http://umji.sjtu.edu.cn">Shanghai Jiao Tong University</a>, advised by <a href="http://wanglab.sjtu.edu.cn/en/content.aspx?info_lb=472&flag=295">Prof. Xudong Wang</a>. 
       </p>
       <p>
       My research interests include both theory and practice. For the theory part, I am interested in large-scale data analysis, linear algebra, optimization, and submodular function. For the practice part, I am interested in design and analysis of machine learning algorithms in distributed or parallel systems. I am currently using <a href="https://spark.apache.org">Apache Spark</a> as my experimental platform.
        </p>
       Here is my <a href="mailto:shanshan@utexas.edu">Email</a> and <a href="../files/CV.pdf">CV</a>.
        </p>
        </td>
        <td width="33%">
        <img src="../images/shanshan.png">
        </td>
      </tr>
      </table>

---

## Current Project

<table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="31%"><img src="../images/onepass.png" alt="pacman" width="160" height="160"></td>
        <td width="69%" valign="center">
        <p>
           We implement a randomized low-rank approximation algorithm in Spark. We also modify the original two-pass algorithm into a one-pass algorithm for direct low-rank approximation of matrix product. We prove a new spectral norm guarantee for the one-pass algorithm.
        </p>
        </td>
      </tr>
      </table>

---

## Publications

<p>
	<b><a href="../files/nips2015.pdf">Sparse and Greedy: Sparsifying Submodular Facility Location Problems</a></b><br>
              Erik Lindgren, Shanshan Wu, and Alex Dimakis<br>
              NIPS workshop OPT2015. <br>
</p>

<p>
	<b><a href="../files/TVT.pdf">Distributed Opportunistic Scheduling with QoS Constraints for Wireless Networks with Hybrid Links</a></b><br>
              Wenguang Mao, Xudong Wang, and Shanshan Wu<br>
              IEEE Transactions on Vehicular Technology, 2015.<br>
              An earlier version appears in Proceedings of the IEEE Globecom, 2013.<br>
</p>

<p>
	<b><a href="../files/MU-MIMO.pdf">Performance Study on a CSMA/CA-Based MAC Protocol for Multi-User MIMO Wireless LANs</a></b><br>
              Shanshan Wu, Wenguang Mao, and Xudong Wang<br>
              IEEE Transactions on Wireless Communications, 2014.<br>
              An earlier version appears in Proceedings of the IEEE Globecom, 2013.<br>
</p>
<p>
	<b><a href="../files/TW-Relay.pdf">Information-theoretic study on routing path selection in two-way relay networks</a></b><br>
              Shanshan Wu, Wenguang Mao, and Xudong Wang<br>
              Proceedings of the IEEE Globecom, 2013. <br>
</p>

---

## Graduate Courses

<p>
- At UT-Austin: Data Mining (EE380L), Large-Scale Optimization (EE381V), Probability and Stochastic Processes (EE381J), Advanced Algorithms (EE381V), Information Theory (EE381K), <a href="http://www.cs.utexas.edu/~vlr/courses/f15.388g/index.html">Algorithms: Techniques/Theory (CS388G)</a>, Advanced Probability in Learning, Inference, and Networks (EE381V).
</p>
<p>
- At SJTU: Linear Systems, Wireless Communication Theory, Computer Networks, Complex Networks, Random Process, Introduction to Coding and Information Theory.
</p>
<p>
- At edX online: <a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x">Scalable Machine Learning (CS190.1x)</a>, <a href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x">Introduction to Big Data with Apache Spark (CS100.1x)</a>.
</p>



